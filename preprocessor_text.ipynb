{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessor_text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWMbdekgYUyKRIDTMhv1aq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcrY4RRgOFa1"
      },
      "source": [
        "#Preprocessor\n",
        "\n",
        "Data cleaning for processing\n",
        "\n",
        "based in the article: [tweet-topic-modeling-part-2-cleaning-and-preprocessing-tweets](https://pub.towardsai.net/tweet-topic-modeling-part-2-cleaning-and-preprocessing-tweets-e3a08a8b1770)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny693tqsIACp"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import gensim\n",
        "import unicodedata\n",
        "import nltk"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE-vcP3KQ1Oj"
      },
      "source": [
        "# define a string of punctuation symbols\n",
        "punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4o5bmwKQBIS"
      },
      "source": [
        "##Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrzqXkdyg6qe"
      },
      "source": [
        "###Remove emojis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cjOo_rtg9OA"
      },
      "source": [
        "def remove_emojis(row):\n",
        "    \"\"\"Takes a string and removes emojis from it\"\"\"\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',row)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1y1J6Srh3gR"
      },
      "source": [
        "###Remove accentuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFwN68nlh_lM"
      },
      "source": [
        "def remove_accentuation(row):\n",
        "  row = ''.join(ch for ch in unicodedata.normalize('NFKD', row) \n",
        "      if not unicodedata.combining(ch))\n",
        "  return row"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg2ClBBFiYJd"
      },
      "source": [
        "###Remove unicode ascii"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iytP10vbibh5"
      },
      "source": [
        "def remove_ascii(row):\n",
        "  \"\"\"Takes a string and removes unicode ascii from it\"\"\"\n",
        "  row = row.encode('ascii', 'ignore').decode()\n",
        "  return row"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbNwASbQQVRc"
      },
      "source": [
        "###Remove links"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSVldK54QDTw"
      },
      "source": [
        "def remove_links(row):\n",
        "  \"\"\"Takes a string and removes web links from it\"\"\"\n",
        "  row = re.sub(r'http\\S+', '', row)   # remove http links\n",
        "  row = re.sub(r'bit.ly/\\S+', '', row)  # remove bitly links\n",
        "  row = row.strip('[link]')   # remove [links]\n",
        "  row = re.sub(r'pic.twitter\\S+','', row)\n",
        "  return row  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq3wjklcRBW8"
      },
      "source": [
        "###Remove users"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4PDWJsfQ-9q"
      },
      "source": [
        "def remove_users(row):\n",
        "  \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
        "  row = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', row)  # remove re-row\n",
        "  row = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', row)  # remove rowed at\n",
        "  return row"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qehnGwzbdN-z"
      },
      "source": [
        "###Remove @ and # from users"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcLcME6FdWKS"
      },
      "source": [
        "def remove_hashtags_simple(row):\n",
        "  \"\"\"Takes a string and removes # and @\"\"\"\n",
        "  row = re.sub('#', '', row)  # remove #\n",
        "  row = re.sub('@', '', row)  # remove @\n",
        "  return row"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OELidlcORghn"
      },
      "source": [
        "###Remove hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSYRlzLVRlPd"
      },
      "source": [
        "def remove_hashtags(row):\n",
        "  \"\"\"Takes a string and removes any hash tags\"\"\"\n",
        "  row = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', row)  # remove hash tags\n",
        "  return row"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ_KrSmZRywi"
      },
      "source": [
        "###Remove audios and videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MjabrU3R6Td"
      },
      "source": [
        "def remove_av(row):\n",
        "  \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
        "  row = re.sub('VIDEO:', '', row)  # remove 'VIDEO:' from start of row\n",
        "  row = re.sub('AUDIO:', '', row)  # remove 'AUDIO:' from start of row\n",
        "  return row"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTAJd-m3SAIf"
      },
      "source": [
        "###Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDOBwg71SD4f"
      },
      "source": [
        "def tokenize(row):\n",
        "  \"\"\"Returns tokenized representation of words in lemma form excluding stopwords\"\"\"\n",
        "  result = []\n",
        "  for token in gensim.utils.simple_preprocess(row):\n",
        "    if token not in gensim.parsing.preprocessing.STOPWORDS \\\n",
        "            and len(token) > 2:  # drops words with less than 3 characters\n",
        "      result.append(lemmatize(token))\n",
        "  return result"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG_N8NWbSQQ9"
      },
      "source": [
        "###lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv707DufSVR0"
      },
      "source": [
        "def lemmatize(token):\n",
        "  \"\"\"Returns lemmatization of a token\"\"\"\n",
        "  return nltk.WordNetLemmatizer().lemmatize(token, pos='v')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs3IIxxzSZiF"
      },
      "source": [
        "##Use functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S-xGiI_TlcJ"
      },
      "source": [
        "###Preprocess row"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c62wDwviUBrN"
      },
      "source": [
        "def preprocess_row(row):\n",
        "  \"\"\"Main master function to clean rows, stripping noisy characters, and tokenizing use lemmatization\"\"\"\n",
        "\n",
        "  nltk.download('wordnet')\n",
        "\n",
        "  row = remove_emojis(row)\n",
        "  row = remove_accentuation(row)\n",
        "  row = remove_ascii(row)\n",
        "  row = remove_users(row)\n",
        "  row = remove_links(row)\n",
        "  row = remove_hashtags(row)\n",
        "  row = remove_av(row)\n",
        "  row = row.lower()  # lower case\n",
        "  row = re.sub('[' + punctuation + ']+', ' ', row)  # strip punctuation\n",
        "  row = re.sub('\\s+', ' ', row)  # remove double spacing\n",
        "  row = re.sub('([0-9]+)', '', row)  # remove numbers\n",
        "  row_token_list = tokenize(row)  # apply lemmatization and tokenization\n",
        "  row = ' '.join(row_token_list)\n",
        "  return row"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz7RdlrsUKk6"
      },
      "source": [
        "###Preprocessor row without tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8bgiRfYUPYH"
      },
      "source": [
        "def preprocess_row_without_tokenize(row):\n",
        "  \"\"\"Main master function to clean rows only without tokenization or removal of stopwords\"\"\"\n",
        "  row = remove_emojis(row)\n",
        "  row = remove_accentuation(row)\n",
        "  row = remove_ascii(row)\n",
        "  row = remove_users(row)\n",
        "  row = remove_links(row)\n",
        "  row = remove_hashtags(row)\n",
        "  row = remove_av(row)\n",
        "  row = row.lower()  # lower case\n",
        "  row = re.sub('[' + punctuation + ']+', ' ', row)  # strip punctuation\n",
        "  row = re.sub('\\s+', ' ', row)  # remove double spacing\n",
        "  row = re.sub('([0-9]+)', '', row)  # remove numbers\n",
        "  row = re.sub('📝 …', '', row)\n",
        "  return row"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp9cp-lbemQ5"
      },
      "source": [
        "###Preprocessor with simple remove # and @"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgc3RRBHexNt"
      },
      "source": [
        "def preprocess_row_simple_hashtags(row):\n",
        "  \"\"\"Main master function to clean rows only without tokenization or removal of stopwords\"\"\"\n",
        "  row = remove_emojis(row)\n",
        "  row = remove_accentuation(row)\n",
        "  row = remove_ascii(row)\n",
        "  row = remove_links(row)\n",
        "  row = remove_hashtags_simple(row)\n",
        "  row = remove_av(row)\n",
        "  row = row.lower()  # lower case\n",
        "  row = re.sub('[' + punctuation + ']+', ' ', row)  # strip punctuation\n",
        "  row = re.sub('\\s+', ' ', row)  # remove double spacing\n",
        "  row = re.sub('([0-9]+)', '', row)  # remove numbers\n",
        "  row = re.sub('📝 …', '', row)\n",
        "  return row"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}